{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8b5f4eb-7397-46f2-8333-7ec68daa03c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to survey_responses.csv\n"
     ]
    }
   ],
   "source": [
    "# reading all the data into a csv file\n",
    "import csv\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "# Make the API call\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": \"Token token=ffa93ab223c71abc7f54d74b0589c4fb3e59bfc1dffd5fb889db4195967e3c41\",\n",
    "    \"Content-Type\": \"application/vnd.api+json\",\n",
    "}\n",
    "\n",
    "\n",
    "# Initialize an empty list to store all survey responses\n",
    "\n",
    "\n",
    "all_survey_responses = []\n",
    "\n",
    "\n",
    "# Iterate through each page\n",
    "\n",
    "\n",
    "for page_number in range(1, 9):  # Assuming there is only one page\n",
    "\n",
    "    url = f\"https://data.g2.com/api/v1/survey-responses?page[number]={page_number}&page[size]=100\"\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "\n",
    "    if response.status_code == 200:\n",
    "\n",
    "        data = response.json()\n",
    "\n",
    "        # Add survey responses to the list\n",
    "\n",
    "        all_survey_responses.extend(data[\"data\"])\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"Failed to fetch data for page {page_number}:\", response.status_code)\n",
    "\n",
    "\n",
    "# Write data to a CSV file\n",
    "\n",
    "\n",
    "filename = \"survey_responses.csv\"\n",
    "\n",
    "\n",
    "with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "\n",
    "    fieldnames = [\n",
    "        \"id\",\n",
    "        \"type\",\n",
    "        \"product_name\",\n",
    "        \"is_public\",\n",
    "        \"slug\",\n",
    "        \"percent_complete\",\n",
    "        \"star_rating\",\n",
    "        \"title\",\n",
    "        \"love\",\n",
    "        \"hate\",\n",
    "        \"verified_current_user\",\n",
    "        \"is_business_partner\",\n",
    "        \"review_source\",\n",
    "        \"votes_up\",\n",
    "        \"votes_down\",\n",
    "        \"votes_total\",\n",
    "        \"user_id\",\n",
    "        \"user_name\",\n",
    "        \"user_image_url\",\n",
    "        \"country_name\",\n",
    "        \"regions\",\n",
    "        \"submitted_at\",\n",
    "        \"updated_at\",\n",
    "        \"moderated_at\",\n",
    "        \"product_id\",\n",
    "        \"reference_user_consent\",\n",
    "    ]\n",
    "\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for response in all_survey_responses:\n",
    "\n",
    "        attributes = response[\"attributes\"]\n",
    "\n",
    "        regions = attributes.get(\"regions\", [])\n",
    "\n",
    "        if isinstance(regions, str):\n",
    "\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"id\": response[\"id\"],\n",
    "                    \"type\": response[\"type\"],\n",
    "                    \"product_name\": attributes.get(\"product_name\", \"Unknown\"),\n",
    "                    \"is_public\": attributes.get(\"is_public\", \"Unknown\"),\n",
    "                    \"slug\": attributes.get(\"slug\", \"Unknown\"),\n",
    "                    \"percent_complete\": attributes.get(\"percent_complete\", \"Unknown\"),\n",
    "                    \"star_rating\": attributes.get(\"star_rating\", \"Unknown\"),\n",
    "                    \"title\": attributes.get(\"title\", \"Unknown\"),\n",
    "                    \"love\": attributes[\"comment_answers\"]\n",
    "                    .get(\"love\", {})\n",
    "                    .get(\"value\", \"Unknown\"),\n",
    "                    \"hate\": attributes[\"comment_answers\"]\n",
    "                    .get(\"hate\", {})\n",
    "                    .get(\"value\", \"Unknown\"),\n",
    "                    \"verified_current_user\": attributes.get(\n",
    "                        \"verified_current_user\", \"Unknown\"\n",
    "                    ),\n",
    "                    \"is_business_partner\": attributes.get(\n",
    "                        \"is_business_partner\", \"Unknown\"\n",
    "                    ),\n",
    "                    \"review_source\": attributes.get(\"review_source\", \"Unknown\"),\n",
    "                    \"votes_up\": attributes.get(\"votes_up\", \"Unknown\"),\n",
    "                    \"votes_down\": attributes.get(\"votes_down\", \"Unknown\"),\n",
    "                    \"votes_total\": attributes.get(\"votes_total\", \"Unknown\"),\n",
    "                    \"user_id\": attributes.get(\"user_id\", \"Unknown\"),\n",
    "                    \"user_name\": attributes.get(\"user_name\", \"Unknown\"),\n",
    "                    \"user_image_url\": attributes.get(\"user_image_url\", \"Unknown\"),\n",
    "                    \"country_name\": attributes.get(\"country_name\", \"Unknown\"),\n",
    "                    \"regions\": regions,\n",
    "                    \"submitted_at\": attributes.get(\"submitted_at\", \"Unknown\"),\n",
    "                    \"updated_at\": attributes.get(\"updated_at\", \"Unknown\"),\n",
    "                    \"moderated_at\": attributes.get(\"moderated_at\", \"Unknown\"),\n",
    "                    \"product_id\": attributes.get(\"product_id\", \"Unknown\"),\n",
    "                    \"reference_user_consent\": attributes.get(\n",
    "                        \"reference_user_consent\", \"Unknown\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        else:\n",
    "\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"id\": response[\"id\"],\n",
    "                    \"type\": response[\"type\"],\n",
    "                    \"product_name\": attributes.get(\"product_name\", \"Unknown\"),\n",
    "                    \"is_public\": attributes.get(\"is_public\", \"Unknown\"),\n",
    "                    \"slug\": attributes.get(\"slug\", \"Unknown\"),\n",
    "                    \"percent_complete\": attributes.get(\"percent_complete\", \"Unknown\"),\n",
    "                    \"star_rating\": attributes.get(\"star_rating\", \"Unknown\"),\n",
    "                    \"title\": attributes.get(\"title\", \"Unknown\"),\n",
    "                    \"love\": attributes[\"comment_answers\"]\n",
    "                    .get(\"love\", {})\n",
    "                    .get(\"value\", \"Unknown\"),\n",
    "                    \"hate\": attributes[\"comment_answers\"]\n",
    "                    .get(\"hate\", {})\n",
    "                    .get(\"value\", \"Unknown\"),\n",
    "                    \"verified_current_user\": attributes.get(\n",
    "                        \"verified_current_user\", \"Unknown\"\n",
    "                    ),\n",
    "                    \"is_business_partner\": attributes.get(\n",
    "                        \"is_business_partner\", \"Unknown\"\n",
    "                    ),\n",
    "                    \"review_source\": attributes.get(\"review_source\", \"Unknown\"),\n",
    "                    \"votes_up\": attributes.get(\"votes_up\", \"Unknown\"),\n",
    "                    \"votes_down\": attributes.get(\"votes_down\", \"Unknown\"),\n",
    "                    \"votes_total\": attributes.get(\"votes_total\", \"Unknown\"),\n",
    "                    \"user_id\": attributes.get(\"user_id\", \"Unknown\"),\n",
    "                    \"user_name\": attributes.get(\"user_name\", \"Unknown\"),\n",
    "                    \"user_image_url\": attributes.get(\"user_image_url\", \"Unknown\"),\n",
    "                    \"country_name\": attributes.get(\"country_name\", \"Unknown\"),\n",
    "                    \"regions\": \"Unknown\",\n",
    "                    \"submitted_at\": attributes.get(\"submitted_at\", \"Unknown\"),\n",
    "                    \"updated_at\": attributes.get(\"updated_at\", \"Unknown\"),\n",
    "                    \"moderated_at\": attributes.get(\"moderated_at\", \"Unknown\"),\n",
    "                    \"product_id\": attributes.get(\"product_id\", \"Unknown\"),\n",
    "                    \"reference_user_consent\": attributes.get(\n",
    "                        \"reference_user_consent\", \"Unknown\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0f0d354-387b-435e-9ede-53a8c23b229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id              type            product_name  is_public  \\\n",
      "0   18532  survey_responses  G2 Marketing Solutions       True   \n",
      "1   67896  survey_responses  G2 Marketing Solutions       True   \n",
      "2  177131  survey_responses  G2 Marketing Solutions       True   \n",
      "3  179859  survey_responses  G2 Marketing Solutions      False   \n",
      "4  179888  survey_responses  G2 Marketing Solutions       True   \n",
      "\n",
      "                                   slug  percent_complete  star_rating  \\\n",
      "0   g2-marketing-solutions-review-18532          0.954545          4.5   \n",
      "1   g2-marketing-solutions-review-67896          0.923077          4.5   \n",
      "2  g2-marketing-solutions-review-177131          0.428571          4.5   \n",
      "3  g2-marketing-solutions-review-179859          1.000000          5.0   \n",
      "4  g2-marketing-solutions-review-179888          1.000000          5.0   \n",
      "\n",
      "                                               title  \\\n",
      "0              G2 helps market and customer validity   \n",
      "1  Recommended for gaining visibility in B2B Mark...   \n",
      "2                                        Credibility   \n",
      "3  Advocately have been instrumental to our revie...   \n",
      "4                                       Game Changer   \n",
      "\n",
      "                                                love  \\\n",
      "0  G2 has helped our customers publicly validate ...   \n",
      "1  It showcases a wide range of vendors with unbi...   \n",
      "2  Great way to build our brand in a new space, b...   \n",
      "3  The end to end service and process has been th...   \n",
      "4  Advocately facilitates a continuous flow of gr...   \n",
      "\n",
      "                                                hate  ...  \\\n",
      "0  Not a lot to dislike.  Be great as G2 becomes ...  ...   \n",
      "1  There aren't enough people open to reviewing p...  ...   \n",
      "2  So far so good, we are just starting to use ad...  ...   \n",
      "3  There is little that I dislike. Understanding ...  ...   \n",
      "4  This is not a true dislike, but the success of...  ...   \n",
      "\n",
      "                                user_id  \\\n",
      "0  6118c789-70c2-4b0d-87b4-e147de083f6f   \n",
      "1  ecf99a0c-5438-463f-afc1-24f1511b2a62   \n",
      "2  18c7b72b-467b-4119-9be9-0e0486ee35e4   \n",
      "3                                   NaN   \n",
      "4  dfeaf03d-0470-4133-8277-e278765e0b5d   \n",
      "\n",
      "                               user_name  \\\n",
      "0                            Ian Moyse ☑   \n",
      "1                        Dakota McKenzie   \n",
      "2                            Paul McGhee   \n",
      "3  Verified User in Information Services   \n",
      "4                        Patrick Edmonds   \n",
      "\n",
      "                                      user_image_url    country_name  regions  \\\n",
      "0  https://media-exp1.licdn.com/dms/image/C4D03AQ...  United Kingdom  Unknown   \n",
      "1  https://images.g2crowd.com/uploads/avatar/imag...   United States  Unknown   \n",
      "2  https://images.g2crowd.com/uploads/avatar/imag...   United States  Unknown   \n",
      "3                  icons/anonymous-avatar-purple.svg             NaN  Unknown   \n",
      "4  https://media-exp1.licdn.com/dms/image/C5603AQ...          Canada  Unknown   \n",
      "\n",
      "                    submitted_at                     updated_at  \\\n",
      "0  2013-12-06T08:51:22.236-06:00  2013-12-06T08:51:22.236-06:00   \n",
      "1  2015-10-26T10:55:33.637-05:00  2015-10-26T10:55:34.284-05:00   \n",
      "2  2016-10-06T14:50:59.131-05:00  2016-10-06T14:50:59.164-05:00   \n",
      "3  2016-10-18T05:13:52.950-05:00  2016-10-18T05:13:52.977-05:00   \n",
      "4  2016-10-18T09:32:37.908-05:00  2016-10-18T09:32:37.933-05:00   \n",
      "\n",
      "                    moderated_at                            product_id  \\\n",
      "0                            NaN  a7d324a4-06eb-4be2-ad8e-65938bce5fd5   \n",
      "1                            NaN  a7d324a4-06eb-4be2-ad8e-65938bce5fd5   \n",
      "2  2016-10-08T14:32:11.744-05:00  a7d324a4-06eb-4be2-ad8e-65938bce5fd5   \n",
      "3  2016-10-18T09:08:20.958-05:00  a7d324a4-06eb-4be2-ad8e-65938bce5fd5   \n",
      "4  2016-10-18T09:51:51.791-05:00  a7d324a4-06eb-4be2-ad8e-65938bce5fd5   \n",
      "\n",
      "  reference_user_consent  \n",
      "0             unanswered  \n",
      "1             unanswered  \n",
      "2             unanswered  \n",
      "3             unanswered  \n",
      "4             unanswered  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the data from survey_responses.csv into a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_survey = pd.read_csv(\"survey_responses.csv\")\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify it's loaded correctly\n",
    "\n",
    "\n",
    "print(df_survey.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d93c781-de95-47fc-8d2c-a8d299dde541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               love  \\\n",
      "0   18532  G2 has helped our customers publicly validate ...   \n",
      "1   67896  It showcases a wide range of vendors with unbi...   \n",
      "2  177131  Great way to build our brand in a new space, b...   \n",
      "3  179859  The end to end service and process has been th...   \n",
      "4  179888  Advocately facilitates a continuous flow of gr...   \n",
      "\n",
      "                                                hate  \n",
      "0  Not a lot to dislike.  Be great as G2 becomes ...  \n",
      "1  There aren't enough people open to reviewing p...  \n",
      "2  So far so good, we are just starting to use ad...  \n",
      "3  There is little that I dislike. Understanding ...  \n",
      "4  This is not a true dislike, but the success of...  \n"
     ]
    }
   ],
   "source": [
    "# Creating a new DataFrame df_pre with only 'id', 'love', and 'hate' columns\n",
    "df_pre = df_survey[[\"id\", \"love\", \"hate\"]]\n",
    "\n",
    "# Display the first few rows of the new DataFrame to verify it's correct\n",
    "print(df_pre.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f28ecef-47c0-49c6-99dc-c9122002ef65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priyadarshis/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyadarshis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyadarshis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                               love  \\\n",
      "0   18532  G2 has helped our customers publicly validate ...   \n",
      "1   67896  It showcases a wide range of vendors with unbi...   \n",
      "2  177131  Great way to build our brand in a new space, b...   \n",
      "3  179859  The end to end service and process has been th...   \n",
      "4  179888  Advocately facilitates a continuous flow of gr...   \n",
      "\n",
      "                                                hate  \\\n",
      "0  Not a lot to dislike.  Be great as G2 becomes ...   \n",
      "1  There aren't enough people open to reviewing p...   \n",
      "2  So far so good, we are just starting to use ad...   \n",
      "3  There is little that I dislike. Understanding ...   \n",
      "4  This is not a true dislike, but the success of...   \n",
      "\n",
      "                                          love_clean  \\\n",
      "0  g2 helped customer publicly validate prospect ...   \n",
      "1  showcase wide range vendor unbiased helpful re...   \n",
      "2  great way build brand new space leveraging use...   \n",
      "3  end end service process highlight experience a...   \n",
      "4  advocately facilitates continuous flow great r...   \n",
      "\n",
      "                                          hate_clean  \n",
      "0  lot dislike great g2 becomes recognised wider ...  \n",
      "1  nt enough people open reviewing product withou...  \n",
      "2  far good starting additional feature linkedin ...  \n",
      "3  little dislike understanding exactly feature m...  \n",
      "4  true dislike success advocately depends least ...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/1436260814.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['love_clean'] = df_pre['love'].apply(preprocess_text)\n",
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/1436260814.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['hate_clean'] = df_pre['hate'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "# for preprocessing such as removing stopwords, punctuation and special characters\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define standard stop words and add custom stop words\n",
    "standard_stop_words = set(stopwords.words(\"english\"))\n",
    "custom_stop_words = {\n",
    "    \"get\",\n",
    "    \"would\",\n",
    "    \"could\",\n",
    "    \"say\",\n",
    "    \"go\",\n",
    "    \"going\",\n",
    "    \"like\",\n",
    "    \"us\",\n",
    "    \"use\",\n",
    "    \"also\",\n",
    "    \"don't\",\n",
    "    \"that\",\n",
    "    \"not\",\n",
    "}\n",
    "all_stop_words = standard_stop_words.union(custom_stop_words)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text.lower())  # Lowercasing during tokenization\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "\n",
    "    words = [re.sub(r\"[^a-zA-Z0-9]\", \"\", word) for word in words]\n",
    "\n",
    "    # Remove empty tokens and stop words (both standard and custom)\n",
    "\n",
    "    words = [word for word in words if word and word not in all_stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Applying the preprocessing function to your DataFrame\n",
    "\n",
    "# Assuming df_pre is your DataFrame with 'love' and 'hate' columns\n",
    "\n",
    "df_pre[\"love_clean\"] = df_pre[\"love\"].apply(preprocess_text)\n",
    "\n",
    "df_pre[\"hate_clean\"] = df_pre[\"hate\"].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# Display the first few rows to verify the preprocessing\n",
    "\n",
    "print(df_pre.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b5e2a63-03eb-43da-9dd4-019e6f67fb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priyadarshis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/2116953302.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['love_pos_tags'] = df_pre['love_clean'].apply(pos_tag_text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       love_pos_tags  \\\n",
      "0  [(g2, NN), (helped, VBD), (customer, NN), (pub...   \n",
      "1  [(showcase, NN), (wide, JJ), (range, NN), (ven...   \n",
      "2  [(great, JJ), (way, NN), (build, VB), (brand, ...   \n",
      "3  [(end, JJ), (end, NN), (service, NN), (process...   \n",
      "4  [(advocately, RB), (facilitates, VBZ), (contin...   \n",
      "\n",
      "                                       hate_pos_tags  \n",
      "0  [(lot, NN), (dislike, VBZ), (great, JJ), (g2, ...  \n",
      "1  [(nt, RB), (enough, RB), (people, NNS), (open,...  \n",
      "2  [(far, RB), (good, JJ), (starting, VBG), (addi...  \n",
      "3  [(little, JJ), (dislike, NN), (understanding, ...  \n",
      "4  [(true, JJ), (dislike, NN), (success, NN), (ad...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/2116953302.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['hate_pos_tags'] = df_pre['hate_clean'].apply(pos_tag_text)\n"
     ]
    }
   ],
   "source": [
    "# pos tagging for finding most frequent parts of speech\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Download the averaged_perceptron_tagger resource if not already downloaded\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "\n",
    "def pos_tag_text(text):\n",
    "\n",
    "    # Tokenize the text\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Perform POS tagging\n",
    "\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "    return pos_tags\n",
    "\n",
    "\n",
    "# Apply POS tagging to the 'love_clean' and 'hate_clean' columns\n",
    "\n",
    "df_pre[\"love_pos_tags\"] = df_pre[\"love_clean\"].apply(pos_tag_text)\n",
    "\n",
    "df_pre[\"hate_pos_tags\"] = df_pre[\"hate_clean\"].apply(pos_tag_text)\n",
    "\n",
    "\n",
    "# Display the first few rows to see the POS tagging\n",
    "\n",
    "print(df_pre[[\"love_pos_tags\", \"hate_pos_tags\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab9f9902-50f1-44ae-9772-783d0df8d35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          love_nouns  \\\n",
      "0  [g2, customer, prospect, pipeline, opportunity...   \n",
      "1  [showcase, range, vendor, review, solution, or...   \n",
      "2  [way, brand, space, success, alex, job, value,...   \n",
      "3  [end, service, process, experience, success, r...   \n",
      "4  [product, service, review, site, service, resu...   \n",
      "\n",
      "                                          hate_nouns  \n",
      "0    [lot, becomes, audience, give, gartners, money]  \n",
      "1  [people, product, incentive, volume, roi, week...  \n",
      "2         [feature, linkedin, integration, evaluate]  \n",
      "3                   [dislike, mean, flow, hindrance]  \n",
      "4                 [dislike, success, service, place]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/2334378485.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['love_nouns'] = df_pre['love_pos_tags'].apply(extract_nouns)\n",
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/2334378485.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['hate_nouns'] = df_pre['hate_pos_tags'].apply(extract_nouns)\n"
     ]
    }
   ],
   "source": [
    "# Extract words that are tagged as singular nouns (NN), plural nouns (NNS),\n",
    "# proper singular nouns (NNP), or proper plural nouns (NNPS)\n",
    "def extract_nouns(pos_tags):\n",
    "\n",
    "    nouns = [word for word, tag in pos_tags if tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]]\n",
    "    return nouns\n",
    "\n",
    "\n",
    "# Apply the function to extract nouns\n",
    "\n",
    "df_pre[\"love_nouns\"] = df_pre[\"love_pos_tags\"].apply(extract_nouns)\n",
    "\n",
    "df_pre[\"hate_nouns\"] = df_pre[\"hate_pos_tags\"].apply(extract_nouns)\n",
    "\n",
    "\n",
    "# Display the DataFrame to verify the noun extraction\n",
    "\n",
    "print(df_pre[[\"love_nouns\", \"hate_nouns\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4b4a45d-f61d-491f-aee4-c5fa05876cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priyadarshis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/priyadarshis/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/priyadarshis/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          hate_clean  \\\n",
      "0  lot dislike great g2 becomes recognised wider ...   \n",
      "1  nt enough people open reviewing product withou...   \n",
      "2  far good starting additional feature linkedin ...   \n",
      "3  little dislike understanding exactly feature m...   \n",
      "4  true dislike success advocately depends least ...   \n",
      "\n",
      "                                       hate_features  \n",
      "0  [lot, great g2 becomes, audience give gartners...  \n",
      "1  [people, product, incentive, unbiased volume, ...  \n",
      "2  [additional feature linkedin integration, eval...  \n",
      "3     [little dislike, feature mean flow, hindrance]  \n",
      "4  [true dislike success, integrated service, place]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/1539805182.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['hate_features'] = df_pre['hate_clean'].apply(extract_noun_phrases).apply(filter_noun_phrases)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import itertools\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "\n",
    "\n",
    "# Define a function to extract noun phrases\n",
    "def extract_noun_phrases(text):\n",
    "    # Tokenize and POS tag the text\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    # Define a chunk grammar, where NP stands for Noun Phrase\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\"\n",
    "\n",
    "    # Create a chunk parser\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "\n",
    "    # Parse the sentence using the defined chunk grammar\n",
    "    tree = cp.parse(tagged)\n",
    "\n",
    "    # Convert the tree to IOB tags, then extract only the noun phrases\n",
    "    iob_tagged = tree2conlltags(tree)\n",
    "    phrases = [\n",
    "        \" \".join(word for word, tag, chunk in group)\n",
    "        for key, group in itertools.groupby(iob_tagged, lambda x: x[2] != \"O\")\n",
    "        if key\n",
    "    ]\n",
    "\n",
    "    return phrases\n",
    "\n",
    "\n",
    "# Filter out common or irrelevant noun phrases\n",
    "common_nouns = {\"customer\", \"company\"}\n",
    "\n",
    "\n",
    "def filter_noun_phrases(phrases):\n",
    "    return [phrase for phrase in phrases if phrase.lower() not in common_nouns]\n",
    "\n",
    "\n",
    "# Apply the functions to the 'hate' column\n",
    "df_pre[\"hate_features\"] = (\n",
    "    df_pre[\"hate_clean\"].apply(extract_noun_phrases).apply(filter_noun_phrases)\n",
    ")\n",
    "\n",
    "# Display the extracted features\n",
    "print(df_pre[[\"hate_clean\", \"hate_features\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f93c98b-ca68-4c16-8fb9-07d0b7411b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priyadarshis/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# We extracted the Hyphenated Adjective-Noun, Verb-Noun, Noun-Noun Compound, Proper Noun followed by a common noun, Proper Noun followed by a common noun, Adjective-Noun Pairing\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "\n",
    "def extract_feature_phrases(text):\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    feature_phrases = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tagged_tokens):\n",
    "\n",
    "        word, tag = tagged_tokens[i]\n",
    "\n",
    "        # Hyphenated Adjective-Noun (e.g., user-friendly interface)\n",
    "\n",
    "        if \"-\" in word and i + 1 < len(tagged_tokens):\n",
    "\n",
    "            next_word, next_tag = tagged_tokens[i + 1]\n",
    "\n",
    "            if next_tag.startswith(\"NN\"):\n",
    "\n",
    "                feature_phrases.append(word + \" \" + next_word)\n",
    "\n",
    "        # Verb-Noun (e.g., improve interface)\n",
    "\n",
    "        if tag.startswith(\"VB\") and i + 1 < len(tagged_tokens):\n",
    "\n",
    "            next_word, next_tag = tagged_tokens[i + 1]\n",
    "\n",
    "            if next_tag.startswith(\"NN\"):\n",
    "\n",
    "                feature_phrases.append(word + \" \" + next_word)\n",
    "\n",
    "        # Noun-Noun Compound (e.g., customer service)\n",
    "\n",
    "        if tag.startswith(\"NN\") and i + 1 < len(tagged_tokens):\n",
    "\n",
    "            next_word, next_tag = tagged_tokens[i + 1]\n",
    "\n",
    "            if next_tag.startswith(\"NN\"):\n",
    "\n",
    "                feature_phrases.append(word + \" \" + next_word)\n",
    "\n",
    "        # Proper Noun followed by a common noun\n",
    "\n",
    "        if tag == \"NNP\":\n",
    "\n",
    "            phrase = [word]\n",
    "\n",
    "            i += 1  # Move to the next word\n",
    "\n",
    "            # Collect a sequence of proper nouns\n",
    "\n",
    "            while i < len(tagged_tokens) and tagged_tokens[i][1] == \"NNP\":\n",
    "\n",
    "                phrase.append(tagged_tokens[i][0])\n",
    "\n",
    "                i += 1\n",
    "\n",
    "            # If the next word is a common noun, add it to the phrase\n",
    "\n",
    "            if i < len(tagged_tokens) and tagged_tokens[i][1] in [\"NN\", \"NNS\"]:\n",
    "\n",
    "                phrase.append(tagged_tokens[i][0])\n",
    "\n",
    "                feature_phrases.append(\" \".join(phrase))\n",
    "\n",
    "        # Adjective-Noun Pairing\n",
    "\n",
    "        elif tag.startswith(\"JJ\"):\n",
    "\n",
    "            for j in range(i + 1, len(tagged_tokens)):\n",
    "\n",
    "                if tagged_tokens[j][1] in [\"NN\", \"NNS\"]:\n",
    "\n",
    "                    phrase = \" \".join([word, tagged_tokens[j][0]])\n",
    "\n",
    "                    feature_phrases.append(phrase)\n",
    "\n",
    "                    break\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return feature_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf41c9a-f472-4bdd-bee4-d5fbbb1ccdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          hate_clean  \\\n",
      "0  lot dislike great g2 becomes recognised wider ...   \n",
      "1  nt enough people open reviewing product withou...   \n",
      "2  far good starting additional feature linkedin ...   \n",
      "3  little dislike understanding exactly feature m...   \n",
      "4  true dislike success advocately depends least ...   \n",
      "\n",
      "                                hate_feature_phrases  \n",
      "0  [great becomes, g2 becomes, wider audience, au...  \n",
      "1  [reviewing product, reviewer volume, honest vo...  \n",
      "2  [good feature, additional feature, feature lin...  \n",
      "3  [little dislike, feature mean, mean flow, slig...  \n",
      "4  [true dislike, dislike success, least service,...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/97379694.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['hate_feature_phrases'] = df_pre['hate_clean'].apply(extract_feature_phrases)\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_pre['hate_clean'] contains the text you want to process\n",
    "df_pre[\"hate_feature_phrases\"] = df_pre[\"hate_clean\"].apply(extract_feature_phrases)\n",
    "\n",
    "# Display the DataFrame to verify the results\n",
    "print(df_pre[[\"hate_clean\", \"hate_feature_phrases\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a09ad38f-f0e4-4511-a5a5-7a096cef7f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      [great becomes, g2 becomes, wider audience, au...\n",
      "1      [reviewing product, reviewer volume, honest vo...\n",
      "2      [good feature, additional feature, feature lin...\n",
      "3      [little dislike, feature mean, mean flow, slig...\n",
      "4      [true dislike, dislike success, least service,...\n",
      "                             ...                        \n",
      "757    [update creation, creation filter, filter data...\n",
      "758    [next search, american search, giant search, t...\n",
      "759                         [come mind, appreciate tool]\n",
      "760    [nt mention, finish recommendation, tiny thing...\n",
      "761    [many feature, paid software, higher list, rev...\n",
      "Name: hate_feature_phrases, Length: 762, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_pre[\"hate_feature_phrases\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed06590d-8e1e-40d9-9a1f-304747112df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'hate_feature_phrases' column has been saved to hate_feature_phrases.csv\n"
     ]
    }
   ],
   "source": [
    "# Select only the 'hate_feature_phrases' column and save it to a CSV file\n",
    "df_pre[[\"hate_feature_phrases\"]].to_csv(\"hate_feature_phrases.csv\", index=False)\n",
    "\n",
    "print(\"The 'hate_feature_phrases' column has been saved to hate_feature_phrases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e04c66da-fc8c-434f-9710-60eb00d8272f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /opt/anaconda3/lib/python3.11/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in /opt/anaconda3/lib/python3.11/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk>=3.8->textblob) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24f3ac02-1cc3-43c2-829e-4e1c051e526c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                hate_feature_phrases  \\\n",
      "0  [great becomes, g2 becomes, wider audience, au...   \n",
      "1  [reviewing product, reviewer volume, honest vo...   \n",
      "2  [good feature, additional feature, feature lin...   \n",
      "3  [little dislike, feature mean, mean flow, slig...   \n",
      "4  [true dislike, dislike success, least service,...   \n",
      "\n",
      "                      hate_feature_phrase_sentiments  \\\n",
      "0                     [0.8, 0.0, 0.0, 0.0, 0.0, 0.0]   \n",
      "1  [0.0, 0.0, 0.6, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, ...   \n",
      "2                          [0.7, 0.0, 0.0, 0.0, 0.0]   \n",
      "3  [-0.1875, -0.3125, -0.3125, -0.16666666666666666]   \n",
      "4                             [0.35, 0.3, -0.3, 0.0]   \n",
      "\n",
      "                       hate_feature_phrases_filtered  \n",
      "0  [g2 becomes, wider audience, audience give, gi...  \n",
      "1  [reviewing product, reviewer volume, unbiased ...  \n",
      "2  [additional feature, feature linkedin, linkedi...  \n",
      "3  [little dislike, feature mean, mean flow, slig...  \n",
      "4                [least service, integrated service]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/2896364429.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['hate_feature_phrase_sentiments'] = df_pre['hate_feature_phrases'].apply(lambda phrases: [get_sentiment(phrase) for phrase in phrases])\n",
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/2896364429.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['hate_feature_phrases_filtered'] = df_pre.apply(lambda row: [phrase for phrase, sentiment in zip(row['hate_feature_phrases'], row['hate_feature_phrase_sentiments']) if sentiment <= 0], axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Removing all the positive sentiments\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# Define a function to get sentiment polarity\n",
    "\n",
    "\n",
    "def get_sentiment(phrase):\n",
    "\n",
    "    return TextBlob(phrase).sentiment.polarity\n",
    "\n",
    "\n",
    "# Assuming 'hate_feature_phrases' is a column with lists of phrases\n",
    "\n",
    "\n",
    "# Apply the sentiment analysis on each phrase in each row's list\n",
    "\n",
    "\n",
    "df_pre[\"hate_feature_phrase_sentiments\"] = df_pre[\"hate_feature_phrases\"].apply(\n",
    "    lambda phrases: [get_sentiment(phrase) for phrase in phrases]\n",
    ")\n",
    "\n",
    "\n",
    "# Filter out positive phrases based on their sentiment polarity\n",
    "\n",
    "\n",
    "df_pre[\"hate_feature_phrases_filtered\"] = df_pre.apply(\n",
    "    lambda row: [\n",
    "        phrase\n",
    "        for phrase, sentiment in zip(\n",
    "            row[\"hate_feature_phrases\"], row[\"hate_feature_phrase_sentiments\"]\n",
    "        )\n",
    "        if sentiment <= 0\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Display the updated DataFrame\n",
    "\n",
    "\n",
    "print(\n",
    "    df_pre[\n",
    "        [\n",
    "            \"hate_feature_phrases\",\n",
    "            \"hate_feature_phrase_sentiments\",\n",
    "            \"hate_feature_phrases_filtered\",\n",
    "        ]\n",
    "    ].head()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b06f98ce-ccf6-4bdc-b3ee-b51f12c6de9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'hate_feature_phrases_filtered' column has been saved to hate_feature_phrases.csv\n"
     ]
    }
   ],
   "source": [
    "# Select only the 'hate_feature_phrases' column and save it to a CSV file\n",
    "df_pre[[\"hate_feature_phrases_filtered\"]].to_csv(\n",
    "    \"hate_feature_phrases_filtered.csv\", index=False\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The 'hate_feature_phrases_filtered' column has been saved to hate_feature_phrases.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c59e919c-3bef-47ac-bb44-6bd1a0448dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['g2 becomes', 'wider audience', 'audience give', 'give gartners', 'run money'], ['reviewing product', 'reviewer volume', 'unbiased volume', 'lower roi', 'costly roi', 'saw roi', 'week launching', 'paid subscription', 'deal lead'], ['additional feature', 'feature linkedin', 'linkedin integration', 'nt evaluate'], ['little dislike', 'feature mean', 'mean flow', 'slight hindrance'], ['least service', 'integrated service']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/6j3r62y50c15h8166drggvw00000gn/T/ipykernel_10736/2036393568.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_pre['hate_feature_phrases_filtered'] = df_pre['hate_feature_phrases_filtered'].apply(lambda x: x if isinstance(x, list) else [])\n"
     ]
    }
   ],
   "source": [
    "# Ensure every cell in 'hate_feature_phrases_filtered' is a list\n",
    "df_pre[\"hate_feature_phrases_filtered\"] = df_pre[\"hate_feature_phrases_filtered\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "# Now, convert the column to a list of lists\n",
    "dataset = df_pre[\"hate_feature_phrases_filtered\"].tolist()\n",
    "\n",
    "# Showing the structure of the dataset\n",
    "print(dataset[:5])  # Print the first 5 entries to check the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df46afb6-20fb-44d5-a35e-3388db7cf7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning the two word features which we have extracted,  we are removing some irrelevant words\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "\n",
    "df = pd.read_csv(\"hate_feature_phrases_filtered.csv\")\n",
    "\n",
    "\n",
    "# Define the words to be removed\n",
    "\n",
    "words_to_remove = [\n",
    "    \"g2\",\n",
    "    \"time\",\n",
    "    \"wish\",\n",
    "    \"know\",\n",
    "    \"half\",\n",
    "    \"hour\",\n",
    "    \"give\",\n",
    "    \"review\",\n",
    "    \"heck\",\n",
    "    \"hey\",\n",
    "    \"people\",\n",
    "    \"coming\",\n",
    "    \"understand\",\n",
    "    \"admin\" \"way\",\n",
    "    \"nothing\",\n",
    "    \"dislike\",\n",
    "    \"np\",\n",
    "    \"try\",\n",
    "    \"come\",\n",
    "    \"lot\",\n",
    "    \"told\",\n",
    "    \"get\",\n",
    "    \"next\",\n",
    "]\n",
    "\n",
    "\n",
    "# Function to filter out rows containing any of the words to be removed\n",
    "\n",
    "\n",
    "def filter_features(row):\n",
    "\n",
    "    features = eval(\n",
    "        row\n",
    "    )  # Assuming the feature phrases are stored as string representation of lists\n",
    "\n",
    "    filtered_features = [\n",
    "        f for f in features if not any(word in f for word in words_to_remove)\n",
    "    ]\n",
    "    return filtered_features\n",
    "\n",
    "\n",
    "# Apply the filter function to each row in the column\n",
    "\n",
    "df[\"hate_feature_phrases_filtered\"] = df[\"hate_feature_phrases_filtered\"].apply(\n",
    "    filter_features\n",
    ")\n",
    "\n",
    "\n",
    "# Write the modified DataFrame back to the CSV file\n",
    "\n",
    "df.to_csv(\"pruned_hate_feature_phrases_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "579f2a3f-713f-4bb3-ba82-6552f9c54109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset (assuming it's a CSV and contains a 'review_id' column)\n",
    "df_original = pd.read_csv(\"survey_responses.csv\")\n",
    "\n",
    "# Ensure that your filtered dataset also has the 'review_id' column\n",
    "# For this example, I'm assuming it's the first column in the df DataFrame\n",
    "df[\"id\"] = df_original[\"id\"]\n",
    "\n",
    "# Now df has the pruned feature phrases and the corresponding review ID\n",
    "# Save the updated DataFrame to a new CSV\n",
    "df[[\"id\", \"hate_feature_phrases_filtered\"]].to_csv(\n",
    "    \"pruned_hate_feature_phrases_filtered.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd227a9b-7ebb-45e1-a4c0-8eb8c9f95d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id         hate_feature      categories\n",
      "0       18532       wider audience           Other\n",
      "1       18532            run money            cost\n",
      "2       67896      unbiased volume           Other\n",
      "3       67896            lower roi             roi\n",
      "4       67896           costly roi             roi\n",
      "...       ...                  ...             ...\n",
      "3282  9473115       change company           Other\n",
      "3283  9473115  company description        security\n",
      "3284  9473651        paid software  tools and tech\n",
      "3285  9473651         ask question           Other\n",
      "3286  9473651           make field           Other\n",
      "\n",
      "[3287 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# categorizing the extracted features into 8 different categories\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define category mapping\n",
    "\n",
    "category_mapping = {\n",
    "    \"roi\": [\"return on investment\", \"profit\", \"revenue\", \"roi\"],\n",
    "    \"social\": [\"social\", \"linkedin log\", \"email\", \"asset\", \"lead\"],\n",
    "    \"Info\": [\n",
    "        \"quality\",\n",
    "        \"experience\",\n",
    "        \"satisfaction\",\n",
    "        \"bug\",\n",
    "        \"issues\",\n",
    "        \"feature\",\n",
    "        \"resource\",\n",
    "        \"info\",\n",
    "        \"missing\",\n",
    "        \"negative\",\n",
    "        \"burden\",\n",
    "        \"granular\",\n",
    "    ],\n",
    "    \"tools and tech\": [\n",
    "        \"tool\",\n",
    "        \"software\",\n",
    "        \"application\",\n",
    "        \"widget\",\n",
    "        \"badge\",\n",
    "        \"grid\",\n",
    "        \"program\",\n",
    "        \"search\",\n",
    "        \"database\",\n",
    "    ],\n",
    "    \"needs\": [\"value\", \"confusing\", \"demand\", \"lack\", \"reporting\", \"interface\", \"doc\"],\n",
    "    \"security\": [\"security\", \"privacy\", \"cybersecurity\", \"ip\"],\n",
    "    \"cost\": [\n",
    "        \"cost\",\n",
    "        \"price\",\n",
    "        \"expense\",\n",
    "        \"money\",\n",
    "        \"charge\",\n",
    "        \"subscription\",\n",
    "        \"expensive\",\n",
    "        \"membership\",\n",
    "    ],\n",
    "    \"landing and other pages\": [\"landing page\", \"page\"],\n",
    "    # Add more categories and associated words as needed\n",
    "}\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "\n",
    "df = pd.read_csv(\"output_1.csv\")  # Change file path as necessary\n",
    "\n",
    "\n",
    "# Create a function to categorize each feature\n",
    "\n",
    "\n",
    "def categorize_feature(text):\n",
    "\n",
    "    for category, keywords in category_mapping.items():\n",
    "\n",
    "        for keyword in keywords:\n",
    "\n",
    "            if keyword in text.lower():\n",
    "                return category\n",
    "\n",
    "    return \"Other\"  # If no category matches\n",
    "\n",
    "\n",
    "# Create a function to categorize each row\n",
    "\n",
    "\n",
    "def categorize_row(row):\n",
    "\n",
    "    category = categorize_feature(row[\"hate_feature\"])\n",
    "    return category\n",
    "\n",
    "\n",
    "# Apply the categorize_row function to create a new column 'categories'\n",
    "\n",
    "df[\"categories\"] = df.apply(categorize_row, axis=1)\n",
    "\n",
    "\n",
    "# Save the categorized data back to a CSV file\n",
    "\n",
    "df.to_csv(\"categorized_data.csv\", index=False)\n",
    "\n",
    "\n",
    "# Display the categorized data\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe09ee63-cfda-443c-b3cf-ac6a015f964c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                categories                                       hate_feature\n",
      "0                     Info  {177131: 'feature linkedin', 179859: 'feature ...\n",
      "1                    Other  {18532: 'wider audience', 67896: 'week launchi...\n",
      "2                     cost  {18532: 'run money', 336870: 'spending money',...\n",
      "3  landing and other pages  {305790: 'corporate page', 329787: 'profile pa...\n",
      "4                    needs  {183103: 'value functionality', 184048: 'ton r...\n",
      "5                      roi  {67896: 'saw roi', 354532: 'roi track', 658177...\n",
      "6                 security  {67896: 'paid subscription', 333833: 'tripadvi...\n",
      "7                   social  {67896: 'deal lead', 297239: 'linkedin log', 3...\n",
      "8           tools and tech  {280373: 'grid question', 288758: 'tool month'...\n"
     ]
    }
   ],
   "source": [
    "# grouping the categorized data that is a key:value pair being stored (id:extracted_feature)\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the categorized data CSV file\n",
    "\n",
    "df = pd.read_csv(\"categorized_data.csv\")  # Change file path as necessary\n",
    "\n",
    "\n",
    "# Group the extracted features by categories and create key-value pairs\n",
    "\n",
    "grouped_features = (\n",
    "    df.groupby(\"categories\")\n",
    "    .apply(lambda x: dict(zip(x[\"id\"], x[\"hate_feature\"])))\n",
    "    .reset_index(name=\"hate_feature\")\n",
    ")\n",
    "\n",
    "\n",
    "# Save the grouped features to a new CSV file\n",
    "\n",
    "grouped_features.to_csv(\"grouped_features.csv\", index=False)\n",
    "\n",
    "\n",
    "# Display the grouped features\n",
    "print(grouped_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "775703f7-6601-448d-ac6e-9c105e60ef82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info                       {177131: 'feature linkedin', 179859: 'feature ...\n",
      "Other                      {184054: 'think anything', 190023: 'found anyt...\n",
      "cost                       {18532: 'run money', 336870: 'spending money',...\n",
      "landing and other pages    {305790: 'corporate page', 329787: 'profile pa...\n",
      "needs                      {183103: 'value functionality', 184048: 'ton r...\n",
      "roi                        {67896: 'saw roi', 354532: 'roi track', 658177...\n",
      "security                   {67896: 'paid subscription', 333833: 'tripadvi...\n",
      "social                     {67896: 'deal lead', 297239: 'linkedin log', 3...\n",
      "tools and tech             {280373: 'grid question', 288758: 'tool month'...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# extracting all the top_features\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read the grouped features CSV file\n",
    "\n",
    "df = pd.read_csv(\"grouped_features.csv\")  # Change file path as necessary\n",
    "\n",
    "\n",
    "# Function to convert string representation of dictionary to dictionary\n",
    "\n",
    "\n",
    "def str_to_dict(s):\n",
    "\n",
    "    try:\n",
    "\n",
    "        return ast.literal_eval(s)\n",
    "\n",
    "    except (SyntaxError, ValueError):\n",
    "\n",
    "        return {}\n",
    "\n",
    "\n",
    "# Convert the 'extracted_feature' column to dictionaries\n",
    "\n",
    "df[\"hate_feature\"] = df[\"hate_feature\"].apply(str_to_dict)\n",
    "\n",
    "\n",
    "# Function to extract top 5 features from each category\n",
    "\n",
    "\n",
    "def extract_top_features(group):\n",
    "\n",
    "    top_features = {}\n",
    "\n",
    "    for category, features_dict in zip(group[\"categories\"], group[\"hate_feature\"]):\n",
    "\n",
    "        # Count the frequency of each feature\n",
    "\n",
    "        feature_count = {}\n",
    "\n",
    "        for feature in features_dict.values():\n",
    "\n",
    "            feature_count[feature] = feature_count.get(feature, 0) + 1\n",
    "\n",
    "        # Sort features by frequency and select the top N\n",
    "\n",
    "        sorted_features = sorted(\n",
    "            feature_count.items(), key=lambda x: x[1], reverse=True\n",
    "        )[:10]\n",
    "\n",
    "        top_features_list = [feature for feature, count in sorted_features]\n",
    "\n",
    "        # Map the top features back to their IDs\n",
    "\n",
    "        feature_to_id = {}\n",
    "\n",
    "        for id, feature in features_dict.items():\n",
    "\n",
    "            if feature in top_features_list:\n",
    "\n",
    "                # Check if the feature is already added; if so, skip\n",
    "\n",
    "                if feature not in feature_to_id.values():\n",
    "\n",
    "                    feature_to_id[id] = feature\n",
    "\n",
    "        top_features[category] = feature_to_id\n",
    "\n",
    "    return pd.Series(top_features)\n",
    "\n",
    "\n",
    "# Extract top 5 features from each category\n",
    "\n",
    "top_features_df = extract_top_features(df)\n",
    "\n",
    "\n",
    "# Save the top features to a new CSV file\n",
    "\n",
    "top_features_df.to_csv(\"top_features.csv\")\n",
    "\n",
    "\n",
    "# Display the top features\n",
    "\n",
    "print(top_features_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
